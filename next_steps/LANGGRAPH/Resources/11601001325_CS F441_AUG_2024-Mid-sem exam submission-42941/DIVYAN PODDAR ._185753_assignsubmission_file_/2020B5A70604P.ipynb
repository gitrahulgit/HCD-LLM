{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install --quiet -U langgraph langchain-community langchain-openai tavily-python\n",
    "%pip install langgraph langgraph-sdk langgraph-checkpoint-sqlite langsmith langchain-community langchain-core langchain-openai langchain-google-vertexai langchain-groq langchain-google-genai notebook tavily-python wikipedia pypdf chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGCHAIN_TRACING_V2=True\n",
    "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "LANGCHAIN_API_KEY=\"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\"\n",
    "LANGCHAIN_PROJECT=\"question3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "\n",
    "def _set_env(var: str):\n",
    "    if not os.environ.get(var):\n",
    "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
    "\n",
    "\n",
    "_set_env(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from typing import Annotated, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "from typing import TypedDict, List, Optional\n",
    "\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph.message import add_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('question.txt', 'r') as file:\n",
    "    question = file.read()\n",
    "with open('model_solution.txt','r') as file:\n",
    "    model_solution = file.read()\n",
    "with open('rubric.txt','r') as file:\n",
    "    rubric = file.read()\n",
    "with open('student_solution.txt','r') as file:\n",
    "    student_solution = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassDetails(TypedDict):\n",
    "    model_code: str\n",
    "    student_code: str\n",
    "    rubric: str\n",
    "    initial_evaluation: str\n",
    "    final_evaluation:  str \n",
    "    extracted_marks: str   \n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    all_class_details: List[ClassDetails]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Annotated, Literal, Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from langgraph.prebuilt import tools_condition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_extractor(state : AgentState):\n",
    "\tprompt = '''You are a class extractor. Given a student's code and a model solution, \n",
    "              your task is to identify the classes that are present in both the student's code and the model solution. \n",
    "              You should return a list of tuples where the first element is the class from the student's code, \n",
    "              and the second element is the corresponding class from the model solution.\n",
    "\t\t\t  model solution = {model_solution}\n",
    "\t\t\t  student solution = {student_solution}\n",
    "             '''\n",
    "\t\n",
    "\tclass extract(BaseModel):\n",
    "\t\tclasses_code: List[Tuple[str, str]] = Field(\n",
    "            description=\"First element of tuple is the class from student code and second element is the class from the model solution\"\n",
    "        )\n",
    "\n",
    "\tmodel = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\tllm_with_tool = model.with_structured_output(extract)\n",
    "\tchain = prompt | llm_with_tool\n",
    "\tresponse = chain.invoke({\"model_solution\" : model_solution , \"student_solution\": student_solution})\n",
    "\textracted_classes = response.extracted_classes  # This is a list of tuples (student_class, model_class)\n",
    "\tall_class_details_new = []\n",
    "\tfor student_class, model_class in extracted_classes:\n",
    "\t\tclass_detail = ClassDetails(\n",
    "            model_code=model_class,\n",
    "            student_code=student_class,\n",
    "            rubric=\"\",  # Populate as needed\n",
    "            initial_evaluation=\"\",  # Populate as needed\n",
    "            final_evaluation=\"\",  # Populate as needed\n",
    "            extracted_marks=\"\"  # Populate as needed\n",
    "        )\n",
    "\t\tall_class_details_new.append(class_detail)\n",
    "\t\n",
    "\treturn {'all_class_details': all_class_details_new} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rubric_extraction_module(state : AgentState): \n",
    "\tall_class_details = state['all_class_details']\n",
    "\tprompt = '''\n",
    "\tGiven the question {question} , rubric {rubric} your task is to generate the rubric for only a particular class of the code\n",
    "\tthe model solution for the class will be provided {model_class}\n",
    "\t'''\n",
    "\tall_class_details_new = []\n",
    "\tfor cls in all_class_details:\n",
    "\t\tmodel_code = cls['model_code']\n",
    "\t\tmodel = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\t\tprompt_final = prompt.format(question = question,rubric=rubric, model_class  = model_code)\n",
    "\t\tresponse = model.invoke(prompt_final)\n",
    "\t\tclass_detail = ClassDetails(\n",
    "\t\t\tmodel_code=model_code,\n",
    "            student_code= cls['student_code'],\n",
    "            rubric= response,  # Populate as needed\n",
    "            initial_evaluation=\"\",  # Populate as needed\n",
    "            final_evaluation=\"\",  # Populate as needed\n",
    "            extracted_marks=\"\"  # Populate as needed\n",
    "\n",
    "\t\t)\n",
    "\t\tall_class_details_new.append(class_detail)\n",
    "\n",
    "\treturn {'all_class_details': all_class_details_new}\n",
    "\n",
    "\t\t\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_evaluation_module(state : AgentState):\n",
    "\tall_class_details = state['all_class_details']\n",
    "\tprompt = '''\n",
    "\tGiven the rubric {rubric} and model solution {model_code} of a class you have to grade the student solution\n",
    "\t{student_code} for the class, I want you to give numerical score as well as comments on correctness, errors and suggestions \n",
    "\tand improvement in student code\n",
    "\t'''\n",
    "\tall_class_details_new = []\n",
    "\tfor cls in all_class_details:\n",
    "\t\tmodel_code = cls['model_code']\n",
    "\t\tstudent_code = cls['student_code']\n",
    "\t\trubric = cls['rubric']\n",
    "\t\tfinal_prompt = prompt.format(rubric = rubric , model_code = model_code , student_code = student_code)\n",
    "\t\tmodel = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\t\tresponse = model.invoke(prompt)\n",
    "\t\tclass_detail = ClassDetails(\n",
    "\t\t\tmodel_code=model_code,\n",
    "            student_code= student_code,\n",
    "            rubric= rubric,  \n",
    "            initial_evaluation= response, \n",
    "            final_evaluation=\"\",\n",
    "            extracted_marks=\"\" \n",
    "\n",
    "\t\t)\n",
    "\t\tall_class_details_new.append(class_detail)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_evaluation_module(state : AgentState):\n",
    "\tall_class_details = state['all_class_details']\n",
    "\tprompt = '''\n",
    "\tGiven the rubric {rubric} and model solution {model_code} of a class you have to grade the student solution\n",
    "\t{student_code} for the class, as well as a first draft of the evaluation {initial_evaluation} , I want you to \n",
    "\tmake the evaluation better make sure it is in line with the rubric\n",
    "\t'''\n",
    "\tall_class_details_new = []\n",
    "\tfor cls in all_class_details:\n",
    "\t\tmodel_code = cls['model_code']\n",
    "\t\tstudent_code = cls['student_code']\n",
    "\t\trubric = cls['rubric']\n",
    "\t\tinitial_evaluation = cls['initial_evaluation']\n",
    "\t\tfinal_prompt = prompt.format(rubric = rubric , model_code = model_code , student_code = student_code, initial_evaluation = initial_evaluation)\n",
    "\t\tmodel = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\t\tresponse = model.invoke(prompt)\n",
    "\t\tclass_detail = ClassDetails(\n",
    "\t\t\tmodel_code=model_code,\n",
    "            student_code= student_code,\n",
    "            rubric= rubric,  \n",
    "            initial_evaluation= initial_evaluation, \n",
    "            final_evaluation= response,\n",
    "            extracted_marks=\"\" \n",
    "\n",
    "\t\t)\n",
    "\t\tall_class_details_new.append(class_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marks_extraction_module(state : AgentState):\n",
    "\tall_class_details = state['all_class_details']\n",
    "\tprompt = '''\n",
    "\tGiven a evaluation, I want you to extract all the marks in the evaluation {final_evaluation}, give the marks in a comma separated \n",
    "\tstring.\n",
    "\t'''\n",
    "\tall_class_details_new = []\n",
    "\tfor cls in all_class_details:\n",
    "\t\tmodel_code = cls['model_code']\n",
    "\t\tstudent_code = cls['student_code']\n",
    "\t\trubric = cls['rubric']\n",
    "\t\tinitial_evaluation = cls['initial_evaluation']\n",
    "\t\tfinal_evaluation = cls['final_evaluation']\n",
    "\t\tfinal_prompt = prompt.format(final_evaluation = final_evaluation)\n",
    "\t\tmodel = ChatOpenAI(temperature=0, model=\"gpt-4-0125-preview\", streaming=True)\n",
    "\t\tresponse = model.invoke(prompt)\n",
    "\t\tclass_detail = ClassDetails(\n",
    "\t\t\tmodel_code=model_code,\n",
    "            student_code= student_code,\n",
    "            rubric= rubric,  \n",
    "            initial_evaluation= initial_evaluation, \n",
    "            final_evaluation= final_evaluation,\n",
    "            extracted_marks= response\n",
    "\n",
    "\t\t)\n",
    "\t\tall_class_details_new.append(class_detail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_marks(comma_separated_integers):\n",
    "    # Split the string by commas and convert each element to an integer\n",
    "    integers_list = [int(x) for x in comma_separated_integers.split(',')]\n",
    "    # Return the sum of the integers in the list\n",
    "    return sum(integers_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import ToolNode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"class_extractor\", class_extractor)\n",
    "workflow.add_node(\"rubric_extraction_module\", rubric_extraction_module)\n",
    "workflow.add_node(\"initial_evaluation_module\", initial_evaluation_module)\n",
    "workflow.add_node(\"review_evaluation_module\", review_evaluation_module)\n",
    "workflow.add_node(\"marks_extraction_module\", marks_extraction_module)\n",
    "\n",
    "workflow.add_edge(START, \"class_extractor\")\n",
    "workflow.add_edge(\"class_extractor\", \"rubric_extraction_module\")\n",
    "workflow.add_edge(\"rubric_extraction_module\", \"initial_evaluation_module\")\n",
    "workflow.add_edge(\"initial_evaluation_module\", \"review_evaluation_module\")\n",
    "workflow.add_edge(\"initial_evaluation_module\", \"marks_extraction_module\")\n",
    "workflow.add_edge(\"marks_extraction_module\", END)\n",
    "\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Image, display\n",
    "\n",
    "try:\n",
    "    display(Image(graph.get_graph(xray=True).draw_mermaid_png()))\n",
    "except Exception:\n",
    "    # This requires some extra dependencies and is optional\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m initial_state \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_class_details\u001b[39m\u001b[38;5;124m'\u001b[39m : []\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m final_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream(initial_state):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     10\u001b[0m         pprint\u001b[38;5;241m.\u001b[39mpprint(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput from node \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/__init__.py:1298\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1293\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1294\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1295\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1296\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1297\u001b[0m     ):\n\u001b[0;32m-> 1298\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1299\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1300\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1301\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1302\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1303\u001b[0m         ):\n\u001b[1;32m   1304\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/utils/runnable.py:409\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/utils/runnable.py:183\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 183\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[68], line 17\u001b[0m, in \u001b[0;36mclass_extractor\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4-0125-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m, streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m llm_with_tool \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwith_structured_output(extract)\n\u001b[0;32m---> 17\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm_with_tool\u001b[49m\n\u001b[1;32m     18\u001b[0m response \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_solution\u001b[39m\u001b[38;5;124m\"\u001b[39m : model_solution , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudent_solution\u001b[39m\u001b[38;5;124m\"\u001b[39m: student_solution})\n\u001b[1;32m     19\u001b[0m extracted_classes \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mextracted_classes  \u001b[38;5;66;03m# This is a list of tuples (student_class, model_class)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py:2989\u001b[0m, in \u001b[0;36mRunnableSequence.__ror__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\n\u001b[1;32m   2979\u001b[0m         other\u001b[38;5;241m.\u001b[39mfirst,\n\u001b[1;32m   2980\u001b[0m         \u001b[38;5;241m*\u001b[39mother\u001b[38;5;241m.\u001b[39mmiddle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2985\u001b[0m         name\u001b[38;5;241m=\u001b[39mother\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   2986\u001b[0m     )\n\u001b[1;32m   2987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\n\u001b[0;32m-> 2989\u001b[0m         \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2990\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst,\n\u001b[1;32m   2991\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle,\n\u001b[1;32m   2992\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast,\n\u001b[1;32m   2993\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   2994\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py:5843\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   5838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5839\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   5840\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a Runnable, callable or dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5841\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5842\u001b[0m     )\n\u001b[0;32m-> 5843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "# prompt: give code to call the agent\n",
    "import pprint\n",
    "\n",
    "initial_state = {\n",
    "    'all_class_details' : []\n",
    "}\n",
    "final_output = None\n",
    "for output in graph.stream(initial_state):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(\"---\")\n",
    "        pprint.pprint(value, indent=2, width=30, depth=None)\n",
    "    pprint.pprint(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m thread \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthread_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m}}\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39mstream({\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall_class_details\u001b[39m\u001b[38;5;124m'\u001b[39m : []\n\u001b[1;32m      4\u001b[0m }, thread):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(s)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/__init__.py:1298\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1288\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1289\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1290\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1293\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1294\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1295\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1296\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1297\u001b[0m     ):\n\u001b[0;32m-> 1298\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1299\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1300\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1301\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1302\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1303\u001b[0m         ):\n\u001b[1;32m   1304\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/utils/runnable.py:409\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 409\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langgraph/utils/runnable.py:183\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 183\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[41], line 17\u001b[0m, in \u001b[0;36mclass_extractor\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m ChatOpenAI(temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4-0125-preview\u001b[39m\u001b[38;5;124m\"\u001b[39m, streaming\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     16\u001b[0m llm_with_tool \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mwith_structured_output(extract)\n\u001b[0;32m---> 17\u001b[0m chain \u001b[38;5;241m=\u001b[39m \u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm_with_tool\u001b[49m\n\u001b[1;32m     18\u001b[0m response \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_solution\u001b[39m\u001b[38;5;124m\"\u001b[39m : model_solution , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudent_solution\u001b[39m\u001b[38;5;124m\"\u001b[39m: student_solution})\n\u001b[1;32m     19\u001b[0m extracted_classes \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mextracted_classes  \u001b[38;5;66;03m# This is a list of tuples (student_class, model_class)\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py:2989\u001b[0m, in \u001b[0;36mRunnableSequence.__ror__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\n\u001b[1;32m   2979\u001b[0m         other\u001b[38;5;241m.\u001b[39mfirst,\n\u001b[1;32m   2980\u001b[0m         \u001b[38;5;241m*\u001b[39mother\u001b[38;5;241m.\u001b[39mmiddle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2985\u001b[0m         name\u001b[38;5;241m=\u001b[39mother\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   2986\u001b[0m     )\n\u001b[1;32m   2987\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\n\u001b[0;32m-> 2989\u001b[0m         \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2990\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfirst,\n\u001b[1;32m   2991\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle,\n\u001b[1;32m   2992\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast,\n\u001b[1;32m   2993\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   2994\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/langchain_core/runnables/base.py:5843\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[0;34m(thing)\u001b[0m\n\u001b[1;32m   5838\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5839\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   5840\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a Runnable, callable or dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5841\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   5842\u001b[0m     )\n\u001b[0;32m-> 5843\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for s in graph.stream({\n",
    "    'all_class_details' : []\n",
    "}, thread):\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
