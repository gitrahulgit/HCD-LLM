{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OK!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sys\n",
    "from typing import Annotated, List\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.messages import BaseMessage\n",
    "import json\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for ANTHROPIC_API_KEY\n",
    "ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY')\n",
    "if not ANTHROPIC_API_KEY:\n",
    "    print(\"Error: ANTHROPIC_API_KEY is not set.\")\n",
    "    print(\"Please set your Anthropic API key in the .env file or as an environment variable.\")\n",
    "    print(\"Example .env file content:\")\n",
    "    print(\"ANTHROPIC_API_KEY=your_api_key_here\")\n",
    "    sys.exit(1)\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = ANTHROPIC_API_KEY\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Java Code Evaluation\"\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[BaseMessage], add_messages]\n",
    "    java_code: str\n",
    "    extracted_classes: List[str]\n",
    "    rubric: dict\n",
    "    initial_evaluation: dict\n",
    "    review_evaluation: dict\n",
    "    marks: dict\n",
    "    total_marks: float\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-haiku-20240307\")\n",
    "\n",
    "def safe_eval(response_content):\n",
    "    try:\n",
    "        return ast.literal_eval(response_content)\n",
    "    except:\n",
    "        try:\n",
    "            return json.loads(response_content)\n",
    "        except:\n",
    "            return {}\n",
    "\n",
    "def class_extraction(state: State):\n",
    "    prompt = f\"\"\"\n",
    "    Extract the class names from the following Java code:\n",
    "    {state['java_code']}\n",
    "    Return only a list of class names.\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    extracted_classes = [class_name.strip() for class_name in response.content.split('\\n') if class_name.strip()]\n",
    "    return {\"extracted_classes\": extracted_classes}\n",
    "\n",
    "def rubric_extraction(state: State):\n",
    "    prompt = \"\"\"\n",
    "    Create a rubric for evaluating Java code. The rubric should include criteria for:\n",
    "    1. Code structure and organization\n",
    "    2. Proper use of object-oriented principles\n",
    "    3. Naming conventions and readability\n",
    "    4. Error handling and input validation\n",
    "    5. Comments and documentation\n",
    "\n",
    "    For each criterion, provide a description and the maximum points available.\n",
    "    Return the rubric as a JSON object.\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    rubric = safe_eval(response.content)\n",
    "    return {\"rubric\": rubric}\n",
    "\n",
    "def initial_evaluation(state: State):\n",
    "    prompt = f\"\"\"\n",
    "    Evaluate the following Java code based on the given rubric:\n",
    "    \n",
    "    Code:\n",
    "    {state['java_code']}\n",
    "    \n",
    "    Rubric:\n",
    "    {state['rubric']}\n",
    "    \n",
    "    Provide an initial evaluation for each criterion in the rubric.\n",
    "    Return the evaluation as a JSON object with the same structure as the rubric.\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    initial_evaluation = safe_eval(response.content)\n",
    "    return {\"initial_evaluation\": initial_evaluation}\n",
    "\n",
    "def review_evaluation(state: State):\n",
    "    prompt = f\"\"\"\n",
    "    Review and refine the initial evaluation of the Java code:\n",
    "    \n",
    "    Code:\n",
    "    {state['java_code']}\n",
    "    \n",
    "    Rubric:\n",
    "    {state['rubric']}\n",
    "    \n",
    "    Initial Evaluation:\n",
    "    {state['initial_evaluation']}\n",
    "    \n",
    "    Provide a refined evaluation, considering any aspects that might have been overlooked.\n",
    "    Return the refined evaluation as a JSON object with the same structure as the initial evaluation.\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    review_evaluation = safe_eval(response.content)\n",
    "    return {\"review_evaluation\": review_evaluation}\n",
    "\n",
    "def marks_extraction(state: State):\n",
    "    prompt = f\"\"\"\n",
    "    Based on the reviewed evaluation, assign marks for each criterion:\n",
    "    \n",
    "    Rubric:\n",
    "    {state['rubric']}\n",
    "    \n",
    "    Reviewed Evaluation:\n",
    "    {state['review_evaluation']}\n",
    "    \n",
    "    Assign marks for each criterion, ensuring they do not exceed the maximum points specified in the rubric.\n",
    "    Return the marks as a JSON object with the same structure as the rubric and evaluations.\n",
    "    \"\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    marks = safe_eval(response.content)\n",
    "    return {\"marks\": marks}\n",
    "\n",
    "def total_marks_calculation(state: State):\n",
    "    total = sum(state['marks'].values())\n",
    "    return {\"total_marks\": total}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes to the graph\n",
    "graph_builder.add_node(\"extract_classes\", class_extraction)\n",
    "graph_builder.add_node(\"extract_rubric\", rubric_extraction)\n",
    "graph_builder.add_node(\"evaluate_initially\", initial_evaluation)\n",
    "graph_builder.add_node(\"review_eval\", review_evaluation)\n",
    "graph_builder.add_node(\"extract_marks\", marks_extraction)\n",
    "graph_builder.add_node(\"calculate_total_marks\", total_marks_calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add edges to the graph\n",
    "graph_builder.add_edge(START, \"extract_classes\")\n",
    "graph_builder.add_edge(\"extract_classes\", \"extract_rubric\")\n",
    "graph_builder.add_edge(\"extract_rubric\", \"evaluate_initially\")\n",
    "graph_builder.add_edge(\"evaluate_initially\", \"review_eval\")\n",
    "graph_builder.add_edge(\"review_eval\", \"extract_marks\")\n",
    "graph_builder.add_edge(\"extract_marks\", \"calculate_total_marks\")\n",
    "graph_builder.add_edge(\"calculate_total_marks\", END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the graph\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution loop\n",
    "while True:\n",
    "    print(\"\\nEnter Java code to evaluate (or 'quit' to exit):\")\n",
    "    java_code = \"\"\n",
    "    while True:\n",
    "        line = input()\n",
    "        if line.lower() == \"quit\":\n",
    "            print(\"Goodbye!\")\n",
    "            sys.exit(0)\n",
    "        if line.strip() == \"\":\n",
    "            break\n",
    "        java_code += line + \"\\n\"\n",
    "    \n",
    "    if java_code.strip():\n",
    "        initial_state = State(\n",
    "            messages=[],\n",
    "            java_code=java_code,\n",
    "            extracted_classes=[],\n",
    "            rubric={},\n",
    "            initial_evaluation={},\n",
    "            review_evaluation={},\n",
    "            marks={},\n",
    "            total_marks=0.0\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            for event in graph.stream(initial_state):\n",
    "                for key, value in event.items():\n",
    "                    if key == \"messages\":\n",
    "                        for message in value:\n",
    "                            if isinstance(message, BaseMessage):\n",
    "                                print(f\"{message.type.capitalize()}: {message.content}\")\n",
    "                    else:\n",
    "                        print(f\"{key.replace('_', ' ').capitalize()}:\")\n",
    "                        print(json.dumps(value, indent=2))\n",
    "                        print()\n",
    "\n",
    "            print(f\"Total Marks: {initial_state['total_marks']}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {str(e)}\")\n",
    "            print(\"Please try again with a different Java code snippet.\")\n",
    "    else:\n",
    "        print(\"No code entered. Please try again.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
